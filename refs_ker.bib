@article{aizerman1964,
author = {Aizerman, A and Braverman, E M and Rozoner, L I},
journal = {Automation and Remote Control},
keywords = {bibtex-import},
pages = {821--837},
title = {{Theoretical foundations of the potential function method in pattern recognition learning}},
volume = {25},
year = {1964}
}
@article{Bakir2004,
annote = {scikit-learn implementation
http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html},
author = {Bakir, GH and Weston, J and Sch{\"{o}}lkopf, B},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakir, Weston, Sch{\"{o}}lkopf - 2004 - Learning to find pre-images.pdf:pdf},
journal = {Advances in neural information {\ldots}},
keywords = {pre-image},
mendeley-tags = {pre-image},
title = {{Learning to find pre-images}},
url = {https://books.google.com.co/books?hl=en{\&}lr={\&}id=0F-9C7K8fQ8C{\&}oi=fnd{\&}pg=PA449{\&}dq=Learning+to+Find+Pre-Images{\&}ots=THJsoZXc3-{\&}sig=dcVKdMBz0hwwft56kz5Artw0m5M},
year = {2004}
}
@incollection{Bakr2004,
address = {Berlin, Heidelberg},
author = {Bakır, Gokhan H. and Zien, Alexander and Tsuda, Koji},
booktitle = {Pattern Recognition},
doi = {10.1007/b99676},
editor = {Rasmussen, Carl Edward and B{\"{u}}lthoff, Heinrich H. and Sch{\"{o}}lkopf, Bernhard and Giese, Martin A.},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakır, Zien, Tsuda - 2004 - Learning to Find Graph Pre-images.pdf:pdf},
isbn = {978-3-540-22945-2},
keywords = {pre-image},
mendeley-tags = {pre-image},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Learning to Find Graph Pre-images}},
url = {http://www.springerlink.com/index/10.1007/b99676},
volume = {3175},
year = {2004}
}
@article{Bhadra2016,
abstract = {In this paper, we introduce the first method that (1) can complete kernel matrices with completely missing rows and columns as opposed to individual missing kernel values, (2) does not require any of the kernels to be complete a priori, and (3) can tackle non-linear kernels. These aspects are necessary in practical applications such as integrating legacy data sets, learning under sensor failures and learning when measurements are costly for some of the views. The proposed approach predicts missing rows by modelling both within-view and between-view relationships among kernel values. We show, both on simulated data and real world data, that the proposed method outperforms existing techniques in the restricted settings where they are available, and extends applicability to new settings.},
archivePrefix = {arXiv},
arxivId = {1602.02518},
author = {Bhadra, Sahely and Kaski, Samuel and Rousu, Juho},
eprint = {1602.02518},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhadra, Kaski, Rousu - 2016 - Multi-view Kernel Completion.pdf:pdf},
keywords = {multimodal,tensor completion},
mendeley-tags = {multimodal,tensor completion},
month = {feb},
title = {{Multi-view Kernel Completion}},
url = {http://arxiv.org/abs/1602.02518},
year = {2016}
}
@article{Cabral2011,
author = {Cabral, RS and la Torre, F De and Costeira, JP and Bernardino, A},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cabral et al. - 2011 - Matrix Completion for Multi-label Image Classification.pdf:pdf},
journal = {NIPS},
title = {{Matrix Completion for Multi-label Image Classification.}},
url = {http://papers.nips.cc/paper/4419-matrix-completionfor-multi-label-image-classification.pdf},
year = {2011}
}
@incollection{Daoqiang2006,
abstract = {In this paper, we extend the original non-negative matrix factorization (NMF) to kernel NMF (KNMF). The advantages of KNMF over NMF are: 1) it could extract more useful features hidden in the original data through some kernel-induced nonlinear mappings; 2) it can deal with data where only relationships (similarities or dissimilarities) between objects are known; 3) it can process data with negative values by using some specific kernel functions (e.g. Gaussian). Thus, KNMF is more general than NMF. To further improve the performance of KNMF, we also propose the SpKNMF, which performs KNMF on sub-patterns of the original data. The effectiveness of the proposed algorithms is validated by extensive experiments on UCI datasets and the FERET face database.},
address = {Berlin, Heidelberg},
author = {Daoqiang, Zhang and Zhi-Hua, Zhou and Songcan, Chen},
booktitle = {PRICAI 2006: Trends in Artificial Intelligence},
doi = {10.1007/978-3-540-36668-3},
editor = {Yang, Qiang and Webb, Geoff},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Daoqiang, Zhi-Hua, Songcan - 2006 - Non-negative Matrix Factorization on Kernels.pdf:pdf},
isbn = {978-3-540-36667-6},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Non-negative Matrix Factorization on Kernels}},
url = {http://www.springerlink.com/index/10.1007/978-3-540-36668-3},
volume = {4099},
year = {2006}
}
@article{Ding,
abstract = {Current nonnegative matrix factorization (NMF) deals with X = FGT type. We provide a systematic analysis and extensions of NMF to the symmetric W = HHT, and the weighted W = HSHT. We show that (1) W = HHT is equivalent to Kernel if-means clustering and the Laplacian-based spectral clustering. (2) X = FGT is equivalent to simultaneous clustering of rows and columns of a bipartite graph. Algorithms are given for computing these symmetric NMFs. Read More: http://epubs.siam.org/doi/abs/10.1137/1.9781611972757.70},
author = {Ding, Chris and He, Xiaofeng and {D. Simon}, Horst},
journal = {Proceedings of the 2005 SIAM International Conference on Data Mining},
language = {en},
title = {{On the Equivalence of Nonnegative Matrix Factorization and Spectral Clustering}},
url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972757.70}
}
@article{Drineas2005,
author = {Drineas, Petros and Mahoney, Michael W.},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Drineas, Mahoney - 2005 - On the Nystr{\"{o}}m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
month = {dec},
pages = {2153--2175},
publisher = {JMLR.org},
title = {{On the Nystr{\"{o}}m Method for Approximating a Gram Matrix for Improved Kernel-Based Learning}},
url = {http://dl.acm.org/citation.cfm?id=1046920.1194916},
volume = {6},
year = {2005}
}
@article{Girosi1993,
abstract = {We had previously shown that regularization principles lead to approximation schemes, as Radial Basis Functions, which are equivalent to networks with one layer of hidden units, called Regularization Networks. In this paper we show that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models, Breiman's hinge functions and some forms of Projection Pursuit Regression. In the probabilistic interpretation of regularization, the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces, and therefore to different types of smoothness assumptions. In the final part of the paper, we also show a relation between activation functions of the Gaussian and sigmoidal type.},
author = {Girosi, Federico and Jones, Michael and Poggio, Tomaso},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Girosi, Jones, Poggio - 1993 - Priors Stabilizers and Basis Functions From Regularization to Radial, Tensor and Additive Splines.pdf:pdf},
keywords = {additivesmodels,multilayer perceptrons,prior knowledge,radial basis functions,regularization theory},
title = {{Priors Stabilizers and Basis Functions: From Regularization to Radial, Tensor and Additive Splines}},
year = {1993}
}
@inproceedings{Graepel2002,
address = {Berlin, Heidelberg},
author = {Graepel, Thore},
booktitle = {Artificial Neural Networks — ICANN 2002},
doi = {10.1007/3-540-46084-5},
editor = {Dorronsoro, Jos{\'{e}} R.},
isbn = {978-3-540-44074-1},
month = {aug},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Kernel Matrix Completion by Semidefinite Programming}},
url = {http://link.springer.com/10.1007/3-540-46084-5},
volume = {2415},
year = {2002}
}
@article{Hardoon2010a,
author = {Hardoon, David R. and Shawe-Taylor, John},
doi = {10.1007/s10994-009-5159-x},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hardoon, Shawe-Taylor - 2010 - Decomposing the tensor kernel support vector machine for neuroscience data with structured labels.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {may},
number = {1-2},
pages = {29--46},
publisher = {Springer US},
title = {{Decomposing the tensor kernel support vector machine for neuroscience data with structured labels}},
url = {http://link.springer.com/10.1007/s10994-009-5159-x},
volume = {79},
year = {2010}
}
@article{Hardoon2010,
annote = {This work address tensor analysis tasks with kernels.},
author = {Hardoon, David R. and Shawe-Taylor, John},
doi = {10.1007/s10994-009-5159-x},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hardoon, Shawe-Taylor - 2010 - Decomposing the tensor kernel support vector machine for neuroscience data with structured labels.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {may},
number = {1-2},
pages = {29--46},
publisher = {Springer US},
title = {{Decomposing the tensor kernel support vector machine for neuroscience data with structured labels}},
url = {http://link.springer.com/10.1007/s10994-009-5159-x},
volume = {79},
year = {2010}
}
@inproceedings{Kulis2006,
address = {New York, New York, USA},
author = {Kulis, Brian and Sustik, M{\'{a}}ty{\'{a}}s and Dhillon, Inderjit},
booktitle = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
doi = {10.1145/1143844.1143908},
isbn = {1595933832},
month = {jun},
pages = {505--512},
publisher = {ACM Press},
title = {{Learning low-rank kernel matrices}},
url = {http://dl.acm.org/citation.cfm?id=1143844.1143908},
year = {2006}
}
@article{Kwok2004,
abstract = {In this paper, we address the problem of finding the pre-image of a feature vector in the feature space induced by a kernel. This is of central importance in some kernel applications, such as on using kernel principal component analysis (PCA) for image denoising. Unlike the traditional method which relies on nonlinear optimization, our proposed method directly finds the location of the pre-image based on distance constraints in the feature space. It is noniterative, involves only linear algebra and does not suffer from numerical instability or local minimum problems. Evaluations on performing kernel PCA and kernel clustering on the USPS data set show much improved performance.},
author = {Kwok, James Tin-yau and Tsang, Ivor Wai-hung},
doi = {10.1109/TNN.2004.837781},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kwok, Tsang - 2004 - The pre-image problem in kernel methods.pdf:pdf;:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kwok, Tsang - 2004 - The pre-image problem in kernel methods(2).pdf:pdf},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Cluster Analysis,Computer Simulation,Computer-Assisted,Computer-Assisted: methods,Decision Support Techniques,Feedback,Image Interpretation,Information Storage and Retrieval,Information Storage and Retrieval: methods,Neural Networks (Computer),Pattern Recognition,Principal Component Analysis,pre-image},
mendeley-tags = {pre-image},
month = {nov},
number = {6},
pages = {1517--25},
pmid = {15565778},
shorttitle = {IEEE Transactions on Neural Networks},
title = {{The pre-image problem in kernel methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15565778},
volume = {15},
year = {2004}
}
@article{Lanckriet2004,
author = {Lanckriet, Gert R. G. and Cristianini, Nello and Bartlett, Peter and Ghaoui, Laurent El and Jordan, Michael I.},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lanckriet et al. - 2004 - Learning the Kernel Matrix with Semidefinite Programming.ps:ps},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {kernel},
mendeley-tags = {kernel},
month = {dec},
pages = {27--72},
publisher = {JMLR.org},
title = {{Learning the Kernel Matrix with Semidefinite Programming}},
url = {http://dl.acm.org/citation.cfm?id=1005332.1005334},
volume = {5},
year = {2004}
}
@inproceedings{Li2005,
author = {Li, Yang and Du, Yangzhou and Lin, Xueyin},
booktitle = {Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
doi = {10.1109/ICCV.2005.131},
isbn = {0-7695-2334-X},
issn = {1550-5499},
keywords = {Data analysis,Face detection,Face recognition,Humans,Image analysis,Image generation,Image recognition,Kernel,Kernel-based multifactor analysis,Lighting,N-mode singular value decomposition,Singular value decomposition,face recognition,facial image,feature extraction,high-quality synthetic face,image recognition,image synthesis,kernel-based factorization,multifactor dataset,singular value decomposition},
language = {English},
pages = {114--119 Vol. 1},
publisher = {IEEE},
title = {{Kernel-based multifactor analysis for image synthesis and recognition}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1541246},
volume = {1},
year = {2005}
}
@inproceedings{Li2012,
abstract = {Non-negative factorization (NMF) has been a popular machine learning method for analyzing microarray data. Kernel approaches can capture more non-linear discriminative features than linear ones. In this paper, we propose a novel kernel NMF (KNMF) approach for feature extraction and classification of microarray data. Our approach is also generalized to kernel high-order NMF (HONMF). Extensive experiments on eight microarray datasets show that our approach generally outperforms the traditional NMF and existing KNMFs. Preliminary experiment on a high-order microarray data shows that our KHONMF is a promising approach given a suitable kernel function.},
author = {Li, Yifeng and Ngom, Alioune},
booktitle = {2012 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)},
doi = {10.1109/CIBCB.2012.6217254},
isbn = {978-1-4673-1191-5},
keywords = {Classification,Clustering algorithms,Equations,Feature Extraction,Feature extraction,Kernel,Kernel Non-Negative Matrix Factorization,Matrix decomposition,Microarray Data,Optimization,Tensile stress,bioinformatics,classification,data classification,feature extraction,genetic algorithms,genetics,kernel nonnegative matrix factorization,learning (artificial intelligence),machine learning method,matrix decomposition,microarray data analysis,nonlinear discriminative features,operating system kernels},
month = {may},
pages = {371--378},
publisher = {IEEE},
shorttitle = {Computational Intelligence in Bioinformatics and C},
title = {{A new Kernel non-negative matrix factorization and its application in microarray data analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6217254},
year = {2012}
}
@article{Liang2010,
abstract = {Nonnegative matrix factorization (NMF) is a technique for analyzing the data structure when nonnegative constraints are imposed. However, NMF aims at minimizing the objective function from the viewpoint of data reconstruction and thus it may produce undesirable performances in classification tasks. In this paper, we develop a novel NMF algorithm (called KDNMF) by optimizing the objective function in a feature space under nonnegative constraints and discriminant constraints. The KDNMF method exploits the geometrical structure of data points and seeks the tradeoff between data reconstruction errors and the geometrical structure of data. The projected gradient method is used to solve KDNMF since directly using the multiplicative update algorithm to update nonnegative matrices is impractical for Gaussian kernels. Experiments on facial expression images and face images are conducted to show the effectiveness of the proposed method.},
author = {Liang, Zhizheng and Li, Youfu and Zhao, Tuo},
doi = {10.1016/j.sigpro.2010.01.019},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang, Li, Zhao - 2010 - Projected gradient method for kernel discriminant nonnegative matrix factorization and the applications.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Discriminant analysis,Feature extraction,Image classification,Kernel function,Nonnegative matrix factorization},
month = {jul},
number = {7},
pages = {2150--2163},
title = {{Projected gradient method for kernel discriminant nonnegative matrix factorization and the applications}},
url = {http://www.sciencedirect.com/science/article/pii/S0165168410000344},
volume = {90},
year = {2010}
}
@article{Mika1998,
author = {Mika, S and Sch{\"{o}}lkopf, B and Smola, AJ and M{\"{u}}ller, KR},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mika et al. - 1998 - Kernel PCA and De-Noising in Feature Spaces.pdf:pdf},
journal = {NIPS},
keywords = {pre-image},
mendeley-tags = {pre-image},
title = {{Kernel PCA and De-Noising in Feature Spaces.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.3384{\&}rep=rep1{\&}type=pdf},
year = {1998}
}
@article{P.HoneineandC.Richard2011,
abstract = {While the nonlinear mapping from the input space to the feature space is central in kernel methods, the reverse mapping from the feature space back to the input space is also of primary interest. This is the case in many applications, including kernel principal component analysis (PCA) for signal and image denoising. Unfortunately, it turns out that the reverse mapping generally does not exist and only a few elements in the feature space have a valid preimage in the input space. The preimage problem consists of finding an approximate solution by identifying data in the input space based on their corresponding features in the high dimensional feature space. It is essentially a dimensionality-reduction problem, and both have been intimately connected in their historical evolution, as studied in this article.},
author = {{P. Honeine and C. Richard}},
doi = {10.1109/MSP.2010.939747},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/P. Honeine and C. Richard - 2011 - Preimage Problem in Kernel-Based Machine Learning.pdf:pdf},
journal = {IEEE Signal Processing Magazine},
keywords = {pre-image},
mendeley-tags = {pre-image},
number = {2},
pages = {77--88},
title = {{Preimage Problem in Kernel-Based Machine Learning}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5714388},
volume = {28},
year = {2011}
}
@inproceedings{PaezTorres2015,
abstract = {Matrix factorization (MF) has shown to be a competitive machine learning strategy for many problems such as dimensionality reduction, latent topic modeling, clustering, dictionary learning and manifold learning, among others. In general, MF is a linear modeling method, so different strategies, most of them based on kernel methods, have been proposed to extend it to non-linearmodeling. However, as with many other kernel methods, memory requirements and computing time limit the application of kernel-based MF methods in large-scale prob- lems. In this paper, we present a new kernel MF (KMF). This method uses a budget, a set of representative points of size p ? n,where n is the size of the training data set, to tackle the memory problem, and uses stochastic gradient descent to tackle the computation time and memory problems. The experimental results show a performance, in particular tasks, comparable to other kernel matrix factorization and clustering methods, and a competitive computing time in large-scale problems. Keywords:},
address = {Cham},
author = {{Paez Torres}, Andres Esteban and Gonzalez, Fabio A.},
booktitle = {Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
doi = {10.1007/978-3-319-25751-8},
editor = {Pardo, Alvaro and Kittler, Josef},
file = {:home/ajaque/Documents/PhD/multi modal/chp{\%}3A10.1007{\%}2F978-3-319-25751-8{\_}78.pdf:pdf},
isbn = {978-3-319-25750-1},
keywords = {Feature space factorization,Kernel matrix factorization,Large-scale learning,kernel NMF},
mendeley-tags = {kernel NMF},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Online Kernel Matrix Factorization}},
url = {http://link.springer.com/10.1007/978-3-319-25751-8},
volume = {9423},
year = {2015}
}
@article{Paisley2010,
author = {Paisley, JW and Carin, L},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paisley, Carin - 2010 - A nonparametric Bayesian model for kernel matrix completion.1146v1:1146v1},
journal = {ICASSP},
title = {{A nonparametric Bayesian model for kernel matrix completion.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.675.2434{\&}rep=rep1{\&}type=pdf},
year = {2010}
}
@misc{Romera-Paredes2013,
author = {Romera-Paredes, Bernardino and Aung, Hane and Bianchi-Berthouze, Nadia and Pontil, Massimiliano},
pages = {1444--1452},
title = {{Multilinear Multitask Learning}},
year = {2013}
}
@article{Rosipal2001,
author = {Rosipal, Roman and Girolami, Mark and Trejo, Leonard J. and Cichocki, Andrzej},
doi = {10.1007/s521-001-8051-z},
issn = {0941-0643},
journal = {Neural Computing {\&} Applications},
month = {dec},
number = {3},
pages = {231--243},
title = {{Kernel PCA for Feature Extraction and De-Noising in Nonlinear Regression}},
url = {http://link.springer.com/10.1007/s521-001-8051-z},
volume = {10},
year = {2001}
}
@book{Shawe-Taylor2004,
abstract = {This book fulfils two major roles: firstly it provides practitioners with a large toolkit of algorithms, kernels and solutions ready to be implemented, suitable for standard pattern discovery problems in field such as bioinformatics, text analysis, image analysis. Secondly it provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so.},
author = {Shawe-Taylor, John and Cristianini, Nello},
isbn = {0521813972},
keywords = {machine learning},
mendeley-tags = {machine learning},
pages = {462},
publisher = {Cambridge University Press},
title = {{Kernel Methods for Pattern Analysis}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=9i0vg12lti4C{\&}pgis=1},
year = {2004}
}
@incollection{Signoretto2010,
author = {Signoretto, Marco and {De Lathauwer}, Lieven and Suykens, Johan A. K.},
doi = {10.1007/978-3-642-15822-3_7},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Signoretto, De Lathauwer, Suykens - 2010 - Kernel-Based Learning from Infinite Dimensional 2-Way Tensors.pdf:pdf},
pages = {59--69},
publisher = {Springer Berlin Heidelberg},
title = {{Kernel-Based Learning from Infinite Dimensional 2-Way Tensors}},
url = {http://link.springer.com/10.1007/978-3-642-15822-3{\_}7},
year = {2010}
}
@article{Signoretto2013,
abstract = {We present a general framework to learn functions in tensor product reproducing kernel Hilbert spaces (TP-RKHSs). The methodology is based on a novel representer theorem suitable for existing as well as new spectral penalties for tensors. When the functions in the TP-RKHS are defined on the Cartesian product of finite discrete sets, in particular, our main problem formulation admits as a special case existing tensor completion problems. Other special cases include transfer learning with multimodal side information and multilinear multitask learning. For the latter case, our kernel-based view is instrumental to derive nonlinear extensions of existing model classes. We give a novel algorithm and show in experiments the usefulness of the proposed extensions.},
archivePrefix = {arXiv},
arxivId = {1310.4977},
author = {Signoretto, Marco and {De Lathauwer}, Lieven and Suykens, Johan A. K.},
eprint = {1310.4977},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Signoretto, De Lathauwer, Suykens - 2013 - Learning Tensors in Reproducing Kernel Hilbert Spaces with Multilinear Spectral Penalties.pdf:pdf},
month = {oct},
title = {{Learning Tensors in Reproducing Kernel Hilbert Spaces with Multilinear Spectral Penalties}},
url = {http://arxiv.org/abs/1310.4977},
year = {2013}
}
@article{Signoretto2011,
abstract = {Tensor-based techniques for learning allow one to exploit the structure of carefully chosen representations of data. This is a desirable feature in particular when the number of training patterns is small which is often the case in areas such as biosignal processing and chemometrics. However, the class of tensor-based models is somewhat restricted and might suffer from limited discriminative power. On a different track, kernel methods lead to flexible nonlinear models that have been proven successful in many different contexts. Nonetheless, a na{\"{i}}ve application of kernel methods does not exploit structural properties possessed by the given tensorial representations. The goal of this work is to go beyond this limitation by introducing non-parametric tensor-based models. The proposed framework aims at improving the discriminative power of supervised tensor-based models while still exploiting the structural information embodied in the data. We begin by introducing a feature space formed by multilinear functionals. The latter can be considered as the infinite dimensional analogue of tensors. Successively we show how to implicitly map input patterns in such a feature space by means of kernels that exploit the algebraic structure of data tensors. The proposed tensorial kernel links to the MLSVD and features an interesting invariance property; the approach leads to convex optimization and fits into the same primal–dual framework underlying SVM-like algorithms.},
author = {Signoretto, Marco and {De Lathauwer}, Lieven and Suykens, Johan A.K.},
doi = {10.1016/j.neunet.2011.05.011},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Signoretto, De Lathauwer, Suykens - 2011 - A kernel-based framework to tensorial data analysis.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
number = {8},
pages = {861--874},
title = {{A kernel-based framework to tensorial data analysis}},
volume = {24},
year = {2011}
}
@article{Signoretto2012,
annote = {This work address tensor analysis tasks with kernels.},
author = {Signoretto, Marco and Olivetti, Emanuele and {De Lathauwer}, Lieven and Suykens, Johan A. K.},
doi = {10.1109/TSP.2012.2186443},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Signoretto et al. - 2012 - Classification of Multichannel Signals With Cumulant-Based Kernels.pdf:pdf},
issn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
month = {may},
number = {5},
pages = {2304--2314},
title = {{Classification of Multichannel Signals With Cumulant-Based Kernels}},
url = {http://ieeexplore.ieee.org/document/6151191/},
volume = {60},
year = {2012}
}
@article{Signoretto2014,
abstract = {We present a framework based on convex optimization and spectral regularization to perform learning when feature observations are multidimensional arrays (tensors). We give a mathematical characterization of spectral penalties for tensors and analyze a unifying class of convex optimization problems for which we present a provably convergent and scalable template algorithm. We then specialize this class of problems to perform learning both in a transductive as well as in an inductive setting. In the transductive case one has an input data tensor with missing features and, possibly, a partially observed matrix of labels. The goal is to both infer the missing input features as well as predict the missing labels. For induction, the goal is to determine a model for each learning task to be used for out of sample prediction. Each training pair consists of a multidimensional array and a set of labels each of which corresponding to related but distinct tasks. In either case the proposed technique exploits precise low multilinear rank assumptions over unknown multidimensional arrays; regularization is based on composite spectral penalties and connects to the concept of Multilinear Singular Value Decomposition. As a by-product of using a tensor-based formalism, our approach allows one to tackle the multi-task case in a natural way. Empirical studies demonstrate the merits of the proposed methods.},
author = {Signoretto, Marco and {Tran Dinh}, Quoc and {De Lathauwer}, Lieven and Suykens, Johan A. K.},
doi = {10.1007/s10994-013-5366-3},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Signoretto et al. - 2014 - Learning with tensors a framework based on convex optimization and spectral regularization.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {kernel,tensors},
mendeley-tags = {kernel,tensors},
month = {mar},
number = {3},
pages = {303--351},
title = {{Learning with tensors: a framework based on convex optimization and spectral regularization}},
url = {http://link.springer.com/10.1007/s10994-013-5366-3},
volume = {94},
year = {2014}
}
@article{Taishin2011,
author = {Taishin, Kin and Tsuyoshi, Kato and Koji, Tsuda and Kiyoshi, Asai},
doi = {10.11234/gi1990.14.516},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taishin et al. - 2011 - Protein Classification via Kernel Matrix Completion.pdf:pdf},
journal = {Genome Informatics},
keywords = {matrix completion},
mendeley-tags = {matrix completion},
month = {jul},
pages = {516},
title = {{Protein Classification via Kernel Matrix Completion}},
url = {https://www.jstage.jst.go.jp/article/gi1990/14/0/14{\_}0{\_}516/{\_}article},
year = {2011}
}
@article{Talwalkar2010,
abstract = {The Nystrom method is an efficient technique to speed up large-scale learning applications by generating low-rank approximations. Crucial to the performance of this technique is the assumption that a matrix can be well approximated by working exclusively with a subset of its columns. In this work we relate this assumption to the concept of matrix coherence and connect matrix coherence to the performance of the Nystrom method. Making use of related work in the compressed sensing and the matrix completion literature, we derive novel coherence-based bounds for the Nystrom method in the low-rank setting. We then present empirical results that corroborate these theoretical bounds. Finally, we present more general empirical results for the full-rank setting that convincingly demonstrate the ability of matrix coherence to measure the degree to which information can be extracted from a subset of columns.},
archivePrefix = {arXiv},
arxivId = {1004.2008},
author = {Talwalkar, Ameet and Rostamizadeh, Afshin},
eprint = {1004.2008},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Talwalkar, Rostamizadeh - 2010 - Matrix Coherence and the Nystrom Method.pdf:pdf},
month = {apr},
title = {{Matrix Coherence and the Nystrom Method}},
url = {http://arxiv.org/abs/1004.2008},
year = {2010}
}
@article{Tsuda2003,
author = {Tsuda, Koji and Akaho, Shotaro and Asai, Kiyoshi},
doi = {10.1162/153244304322765649},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsuda, Akaho, Asai - 2003 - The em algorithm for kernel matrix completion with auxiliary data.pdf:pdf},
issn = {0003-6951},
journal = {The Journal of Machine Learning Research},
month = {dec},
pages = {67--81},
publisher = {JMLR.org},
title = {{The em algorithm for kernel matrix completion with auxiliary data}},
url = {http://dl.acm.org/citation.cfm?id=945365.945369},
volume = {4},
year = {2003}
}
@article{Williams2005,
author = {Williams, D and Carin, L},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams, Carin - 2005 - Analytical kernel matrix completion with incomplete multi-view data.pdf:pdf},
journal = {Proceedings of the ICML Workshop on  {\ldots}},
title = {{Analytical kernel matrix completion with incomplete multi-view data}},
url = {http://www.stefan-rueping.de/publications/rueping-scheffer-2005-a.pdf{\#}page=80},
year = {2005}
}
@article{Xu2011b,
archivePrefix = {arXiv},
arxivId = {1108.6296},
author = {Xu, Zenglin and Yan, Feng and Yuan and Qi},
eprint = {1108.6296},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2011 - Infinite Tucker Decomposition Nonparametric Bayesian Models for Multiway Data Analysis.pdf:pdf},
title = {{Infinite Tucker Decomposition: Nonparametric Bayesian Models for Multiway Data Analysis}},
year = {2011}
}
@article{Xu2011,
abstract = {Tensor decomposition is a powerful computational tool for multiway data analysis. Many popular tensor decomposition approaches---such as the Tucker decomposition and CANDECOMP/PARAFAC (CP)---amount to multi-linear factorization. They are insufficient to model (i) complex interactions between data entities, (ii) various data types (e.g. missing data and binary data), and (iii) noisy observations and outliers. To address these issues, we propose tensor-variate latent nonparametric Bayesian models, coupled with efficient inference methods, for multiway data analysis. We name these models InfTucker. Using these InfTucker, we conduct Tucker decomposition in an infinite feature space. Unlike classical tensor decomposition models, our new approaches handle both continuous and binary data in a probabilistic framework. Unlike previous Bayesian models on matrices and tensors, our models are based on latent Gaussian or {\$}t{\$} processes with nonlinear covariance functions. To efficiently learn the InfTucker from data, we develop a variational inference technique on tensors. Compared with classical implementation, the new technique reduces both time and space complexities by several orders of magnitude. Our experimental results on chemometrics and social network datasets demonstrate that our new models achieved significantly higher prediction accuracy than the most state-of-art tensor decomposition},
annote = {This work address tensor analysis tasks with kernels.},
archivePrefix = {arXiv},
arxivId = {1108.6296},
author = {Xu, Zenglin and Yan, Feng and Yuan and Qi},
eprint = {1108.6296},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2011 - Infinite Tucker Decomposition Nonparametric Bayesian Models for Multiway Data Analysis.pdf:pdf},
month = {aug},
title = {{Infinite Tucker Decomposition: Nonparametric Bayesian Models for Multiway Data Analysis}},
url = {http://arxiv.org/abs/1108.6296},
year = {2011}
}
@article{Xu2011a,
abstract = {Tensor decomposition is a powerful computational tool for multiway data analysis. Many popular tensor decomposition approaches---such as the Tucker decomposition and CANDECOMP/PARAFAC (CP)---amount to multi-linear factorization. They are insufficient to model (i) complex interactions between data entities, (ii) various data types (e.g. missing data and binary data), and (iii) noisy observations and outliers. To address these issues, we propose tensor-variate latent nonparametric Bayesian models, coupled with efficient inference methods, for multiway data analysis. We name these models InfTucker. Using these InfTucker, we conduct Tucker decomposition in an infinite feature space. Unlike classical tensor decomposition models, our new approaches handle both continuous and binary data in a probabilistic framework. Unlike previous Bayesian models on matrices and tensors, our models are based on latent Gaussian or {\$}t{\$} processes with nonlinear covariance functions. To efficiently learn the InfTucker from data, we develop a variational inference technique on tensors. Compared with classical implementation, the new technique reduces both time and space complexities by several orders of magnitude. Our experimental results on chemometrics and social network datasets demonstrate that our new models achieved significantly higher prediction accuracy than the most state-of-art tensor decomposition},
archivePrefix = {arXiv},
arxivId = {1108.6296},
author = {Xu, Zenglin and Yan, Feng and Yuan and Qi},
eprint = {1108.6296},
month = {aug},
title = {{Infinite Tucker Decomposition: Nonparametric Bayesian Models for Multiway Data Analysis}},
url = {http://arxiv.org/abs/1108.6296},
year = {2011}
}
@article{Zhao2013,
author = {Zhao, Qibin and Zhou, Guoxu and Adali, Tulay and Zhang, Liqing and Cichocki, Andrzej},
doi = {10.1109/MSP.2013.2255334},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2013 - Kernelization of Tensor-Based Models for Multiway Data Analysis Processing of Multidimensional Structured Data.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
month = {jul},
number = {4},
pages = {137--148},
title = {{Kernelization of Tensor-Based Models for Multiway Data Analysis: Processing of Multidimensional Structured Data}},
url = {http://ieeexplore.ieee.org/document/6530728/},
volume = {30},
year = {2013}
}
@article{Zhu,
author = {Zhu, F and Honeine, P},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Honeine - Unknown - Pareto front of bi-objective kernel-based nonnegative matrix factorization.pdf:pdf},
journal = {elen.ucl.ac.be},
keywords = {NMF},
mendeley-tags = {NMF},
title = {{Pareto front of bi-objective kernel-based nonnegative matrix factorization}},
url = {https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-59.pdf}
}
@inproceedings{Zhu2015,
author = {Zhu, Fei and Honeine, Paul},
booktitle = {2015 23rd European Signal Processing Conference (EUSIPCO)},
doi = {10.1109/EUSIPCO.2015.7362811},
file = {:home/ajaque/Documents/Machine Learning/Kernel Methods/kNMF/papers/07362811.pdf:pdf},
isbn = {978-0-9928-6263-3},
keywords = {Computational complexity,Encoding,Europe,Gaussian kernel,Gaussian processes,Kernel,Linear programming,NMF,Nonnegative matrix factorization,Signal processing,Stochastic processes,data analysis,dimension-reduction,geophysical image processing,gradient methods,hyperspectral imaging,hyperspectral unmixing,kernel,kernel machine,kernel machines,matrix decomposition,minibatch scheme,nonlinear kernel-based NMF,online learning,online nonnegative matrix factorization,stochastic gradient descent scheme,stochastic processes,unmixing hyperspectral imaging},
language = {English},
mendeley-tags = {NMF,kernel},
month = {aug},
pages = {2381--2385},
publisher = {IEEE},
title = {{Online nonnegative matrix factorization based on kernel machines}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=7362811},
year = {2015}
}
@article{Zhu2014,
abstract = {The nonnegative matrix factorization (NMF) is widely used in signal and image processing, including bio-informatics, blind source separation and hyperspectral image analysis in remote sensing. A great challenge arises when dealing with a nonlinear formulation of the NMF. Within the framework of kernel machines, the models suggested in the literature do not allow the representation of the factorization matrices, which is a fallout of the curse of the pre-image. In this paper, we propose a novel kernel-based model for the NMF that does not suffer from the pre-image problem, by investigating the estimation of the factorization matrices directly in the input space. For different kernel functions, we describe two schemes for iterative algorithms: an additive update rule based on a gradient descent scheme and a multiplicative update rule in the same spirit as in the Lee and Seung algorithm. Within the proposed framework, we develop several extensions to incorporate constraints, including sparseness, smoothness, and spatial regularization with a total-variation-like penalty. The effectiveness of the proposed method is demonstrated with the problem of unmixing hyperspectral images, using well-known real images and results with state-of-the-art techniques.},
annote = {Very relevant, check},
archivePrefix = {arXiv},
arxivId = {1407.4420{\#}},
author = {Zhu, Fei and Honeine, Paul and Kallas, Maya},
eprint = {1407.4420{\#}},
file = {:home/ajaque/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Honeine, Kallas - 2014 - Kernel Nonnegative Matrix Factorization Without the Curse of the Pre-image - Application to Unmixing Hyper.pdf:pdf},
month = {jul},
pages = {13},
title = {{Kernel Nonnegative Matrix Factorization Without the Curse of the Pre-image - Application to Unmixing Hyperspectral Images}},
url = {http://arxiv.org/abs/1407.4420{\#}},
year = {2014}
}
